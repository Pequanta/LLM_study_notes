{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: column; justify-content: center; align-items: center; padding: 4rem\">\n",
    "    <u><h1 style=\"margin: 0 auto\">A General overview on LLMs</h1></u>\n",
    "    <div style=\"border: 1px solid white; width: 50%\">\n",
    "        <p>\n",
    "            With being a category of Foundation models, LLMs are trained on an immense amount of data that make them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.In general, they are designed to understand and generate text like a human based on the types of data they are trained on.\n",
    "        </p>\n",
    "        <ol>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Building</li>\n",
    "            <span>The building stage of LLM development consists of Data prepartion & sampling , Attention Mechanism and LLM architecture.The main step of building LLM is designing and implementing it's architecture.Here tokenization schemes, Encoding Positions ,Attention mechanism , activation functions, Layer normalization and other crucial components are determined.Based on this compoenents there exists differents variants of architecture. Forexample, differing Attention mechanism result in different architecture such as: Encoder Decoder, Causal Decoder, Prefix-decoder and Mixture Decoder.\n",
    "            Since the LLMs demand vast amount of data, another critical step is data preparation and sampling.</span>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Pre-training</li>\n",
    "            <span>Here the model is trained in unsupervised/self-supervised manner on enormous amount of data to gain the ability in understanding and generating texts. This stage produces a base model which has the ability to understand and generate texts. The main feature of this basemodel is to predict the next word based on the current input text.The base model resulted from pre-training is characterized by General-purpose knowledge, Task agnostic nature, large-scale parameter and etc. Even though it lacks performance superiority on specific tasks, compared to its Fine-tuned counterparts, it is flexible and covers wide range of information.However, small fine-tuned model , in general, outperforms base model</span>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Fine-Tuning</li>\n",
    "            <span>This stage focuses on tailoring the general purpose base model to specific task oriented dataset. Here the pre-trainined weights are nudged to increase the base model's performance on the targetd task.</span>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Prompt-Engineering</li>\n",
    "            <span>After the fine-tuning stage, the Prompt stage ensures the output provided by the model is structured by specifying the desired output context,goal or constraints to the model with different prompts.With this step, the model is directed to achieve specific objectives, to optmize it's performance and to enhance the overall user experience. This stage consists of Zero-shot learning -where the model is required to understand and respond to the task provided with natural language description without any example- and few-shots learning- where the model is provided with example to generate a response.</span>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Retrival-Agumented generation(RAG)</li>\n",
    "            <span>\n",
    "            RAG, enhances the accuracey and reliablity of fine-tuned models with facts fetched from external resoures.\n",
    "            </span>\n",
    "        </ol>\n",
    "    </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They pass on successive steps of development before attaining their ready to be used state.This development steps include building,  pretraining and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
