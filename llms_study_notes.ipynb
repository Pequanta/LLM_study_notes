{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: column; justify-content: center; align-items: center; padding: 1rem\">\n",
    "    <u><h1 style=\"margin: 1rem auto\">A General overview on LLMs</h1></u>\n",
    "    <div style=\"border: 0.1rem solid white; width: 100%; padding: 3rem; border-radius: 1rem\">\n",
    "        <h5>With being a category of Foundation models, LLMs are trained on an immense amount of data that make them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.In general, they are designed to understand and generate text like a human based on the types of data they are trained on.</h5>\n",
    "        <h3>\n",
    "            They pass on successive steps of development before attaining their ready to be used state.This development steps include building,  pretraining and fine-tuning.\n",
    "        </h3>\n",
    "        <ol>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Building</li>\n",
    "            <h3>The building stage of LLM development consists of Data prepartion & sampling , Attention Mechanism and LLM architecture.The main step of building LLM is designing and implementing it's architecture.Here tokenization schemes, Encoding Positions ,Attention mechanism , activation functions, Layer normalization and other crucial components are determined.Based on this compoenents there exists differents variants of architecture. Forexample, differing Attention mechanism result in different architecture such as: Encoder Decoder, Causal Decoder, Prefix-decoder and Mixture Decoder.\n",
    "            Since the LLMs demand vast amount of data, another critical step is data preparation and sampling.</h3>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Pre-training</li>\n",
    "            <h3>Here the model is trained in unsupervised/self-supervised manner on enormous amount of data to gain the ability in understanding and generating texts. This stage produces a base model which has the ability to understand and generate texts. The main feature of this basemodel is to predict the next word based on the current input text.The base model resulted from pre-training is characterized by General-purpose knowledge, Task agnostic nature, large-scale parameter and etc. Even though it lacks performance superiority on specific tasks, compared to its Fine-tuned counterparts, it is flexible and covers wide range of information.However, small fine-tuned model , in general, outperforms base model</h3>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Fine-Tuning</li>\n",
    "            <h3>This stage focuses on tailoring the general purpose base model to specific task oriented dataset. Here the pre-trainined weights are nudged to increase the base model's performance on the targetd task.</h3>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Prompt-Engineering</li>\n",
    "            <h3>After the fine-tuning stage, the Prompt stage ensures the output provided by the model is structured by specifying the desired output context,goal or constraints to the model with different prompts.With this step, the model is directed to achieve specific objectives, to optmize it's performance and to enhance the overall user experience. This stage consists of Zero-shot learning -where the model is required to understand and respond to the task provided with natural language description without any example- and few-shots learning- where the model is provided with example to generate a response.</h3>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Retrival-Agumented generation(RAG)</li>\n",
    "            <h3>\n",
    "            RAG, enhances the accuracey and reliablity of fine-tuned models with facts fetched from external resoures.\n",
    "            </h3>\n",
    "        </ol>\n",
    "        <u style=\"margin: 0; padding: 0\"><h2>Fine-tuning</h2></u>\n",
    "        <h3>Fine-tuning employes different mechanisms to prepare the base model for specific task including:</h3>\n",
    "        <li style=\"font-size: 1.5rem;font-weight: 900\"> Supervised fine-tuning</li>\n",
    "        <h3 style=\"margin-left: 3rem\">where the model is further trained on a labeled dataset specific to the targeted task</h3>\n",
    "        <li style=\"font-size: 1.5rem;font-weight: 900\">Few-shot learning </li>\n",
    "        <h3 style=\"margin-left: 3rem\">where the demand for large labeled dataset is avoided by providing few examples of the required task at the beginning of the input query </h3>\n",
    "        <li style=\"font-size: 1.5rem;font-weight: 900\">Reinforcement learning</li>\n",
    "        <h3 style=\"margin-left: 3rem\">where the demand for large labeled dataset is avoided by providing few examples of the required task at the beginning of the input query </h3>\n",
    "        <h3>There are different styles to fine-tune LLMs. To mention a general ones</h3>\n",
    "        <ol>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Transfer Learning</li>\n",
    "            <h3>This method uses pre-trained models from one machine learning task or dataset to improve performance and generalizability on a related task or dataset.</h3>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Aligning tuning</li>\n",
    "            <h3>\n",
    "            This refers to process of modifying or configuring a pre-existing LLM to behave safely and ethically according to human values. It aims to modify the model’s behavior without changing its core capabilities or knowledge base.Involves adding constarints or objects to guid the model's outputs.\n",
    "            </h3>\n",
    "            <li style=\"font-size: 1.5rem;font-weight: 900\">Instruction tuning</li>\n",
    "            <h3>This is a method where a model is trained on instruction formatted data, comprised of multi-task data in plain natural language with instruction and an input output pair.</h3>\n",
    "        </ol>\n",
    "    </div>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "<ol>\n",
    "    <li><b>Mathav Raj J, Kushala VM, Harikrishna Warrier,Yogesh GuptaFine(2024). </b><i>Tuning LLMs For Enterprise: Practical guidlines and recommendations</i></li>\n",
    "    <li><b>Humza Naveeda , Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g , Muhammad Usmanf,g , Naveed Akhtarh,j ,\n",
    "Nick Barnesi , Ajmal Mianj(2024). </b><i>A Comprehensive Overview of Large Language Models</i></li>\n",
    "    <li><b>IBM topics. </b><i>What is instruction tuning?</i></li>\n",
    "    <li><b>IBM topics. </b><i>What are LLMs?</i></li>\n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
